import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import LabelEncoder

def perform_linear_regression(X, y, test_size, random_state, num_iterations, learning_rate):
    """
    Function to perform Linear Regression and display results.
    """
    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    # Initialize theta for gradient descent
    X_train_np = np.array(X_train)
    y_train_np = np.array(y_train).reshape(-1, 1)
    X_train_np = np.insert(X_train_np, 0, 1, axis=1)
    theta = np.zeros((X_train_np.shape[1], 1))

    # Perform gradient descent
    theta, J_history = gradient_descent(X_train_np, y_train_np, theta, learning_rate, num_iterations)

    # Prepare X_test for prediction
    X_test_np = np.array(X_test)
    X_test_np = np.insert(X_test_np, 0, 1, axis=1)

    # Calculate predictions
    y_pred = X_test_np.dot(theta)

    # Calculate and display metrics
    mse = mean_squared_error(y_test, y_pred)
    st.write(f"Mean Squared Error: {mse}")
    st.write("Model Coefficients:")
    st.write(dict(zip(X.columns, theta.flatten())))

    # Plot gradient descent
    plot_gradient_descent(J_history)

def plot_gradient_descent(J_history):
    """
    Function to plot the cost function over iterations during gradient descent.
    """
    fig, ax = plt.subplots()
    ax.plot(range(len(J_history)), J_history, '-b', linewidth=2)
    ax.set_xlabel('Iterations')
    ax.set_ylabel('Cost')
    ax.set_title('Gradient Descent: Cost vs Iterations')
    st.pyplot(fig)

def gradient_descent(X, y, theta, alpha, num_iterations):
    """
    Gradient Descent function to minimize the cost function.
    """
    m = len(y)
    J_history = []

    for i in range(num_iterations):
        theta = theta - (alpha / m) * (X.T.dot(X.dot(theta) - y))
        cost = (1 / (2 * m)) * np.sum((X.dot(theta) - y) ** 2)
        J_history.append(cost)

    return theta, J_history